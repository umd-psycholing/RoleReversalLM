# -*- coding: utf-8 -*-
"""Top_predictions.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1M3SggTpcoG2f2ncSq5KAAQsrvc1D-qam
"""

from google.colab import drive
drive.mount('/content/drive')

"""# Install and load packages"""

import torch
from torch.utils.data import DataLoader
from torch.nn import functional as F

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import scipy.stats as stats
from scipy.stats import pearsonr
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.metrics import classification_report
from sklearn.preprocessing import StandardScaler

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report
from sklearn.preprocessing import StandardScaler

pip install minicons

pip install bertviz

pip install sacremoses

from minicons import scorer
from minicons import cwe
from minicons.utils import character_span # for demonstrating.

from transformers import AutoModelForCausalLM, AutoTokenizer
# from transformers import GPT2Tokenizer, GPT2Model, utils, GPT2LMHeadModel
# from transformers import BertTokenizer, BertForMaskedLM, BertModel

from bertviz import model_view, head_view
from bertviz.util import num_layers

# import random
# import csv
# import re
# import tensorflow as tf
# from statsmodels.stats.proportion import proportions_ztest

# import os

# # Warning: This will download a 550mb model file if you do not already have it!
# model = scorer.IncrementalLMScorer('gpt2', 'cpu')

# gpt2 = AutoModelForCausalLM.from_pretrained('gpt2-medium', return_dict=True)
# gpt2_tokenizer = AutoTokenizer.from_pretrained('gpt2-medium', use_fast=True)
# model = scorer.IncrementalLMScorer(gpt2, tokenizer=gpt2_tokenizer, device='cpu')

# mlm_model = scorer.MaskedLMScorer('bert-base-uncased', 'cpu')

"""# Define helper functions"""

# model = AutoModelForCausalLM.from_pretrained('gpt2', return_dict=True)
# tokenizer = AutoTokenizer.from_pretrained('gpt2', use_fast=True)

def get_top_predictions(text):
    # Encode the input text
    input_ids = tokenizer.encode(text, return_tensors='pt')

    # Get logits from the model
    with torch.no_grad():
        outputs = model(input_ids)
        logits = outputs.logits

    # Extract probabilities for the last token
    last_logits = logits[0, -1, :]
    probs = torch.softmax(last_logits, dim=-1)

    # Get the top 5 predictions
    top_probs, top_indices = torch.topk(probs, 5)
    top_tokens = [tokenizer.decode([idx]).strip() for idx in top_indices]
    return ', '.join(top_tokens)  # Join predictions into a single string

def check_target_in_predictions(predictions, target):
    # Split predictions into a list and check if target is in the list
    return 1 if target in predictions.split(', ') else 0

def check_target_is_top_prediction(predictions, target):
    # Split predictions into a list and check if target is in the list
    return 1 if target in predictions.split(', ')[0] else 0

# # Apply the function to each sentence in the column
# df['top_pred_5'] = df['context_had'].apply(get_top_predictions)

# bert_model = AutoModelForCausalLM.from_pretrained('bert-base-uncased', return_dict=True)
# bert_tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased', use_fast=True)

def get_predictions_with_mask(text, target_word):
    # Replace the target word with the mask token
    # We use a case-insensitive replacement while keeping the original text case
    words = text.split()
    masked_text = ' '.join([bert_tokenizer.mask_token if word.lower() == target_word.lower() else word for word in words])
    input_ids = bert_tokenizer.encode(masked_text, return_tensors='pt')

    # Get logits from the model
    with torch.no_grad():
        outputs = bert_model(input_ids)
        logits = outputs.logits

    # Extract probabilities for the mask token
    mask_token_index = torch.where(input_ids == bert_tokenizer.mask_token_id)[1]
    # Multiple mask tokens might be present; handle this case
    mask_logits = logits[0, mask_token_index, :]
    top_preds_per_mask = []
    for logits in mask_logits:
        probs = torch.softmax(logits, dim=-1)
        top_probs, top_indices = torch.topk(probs, 5)
        top_tokens = [bert_tokenizer.decode([idx]).strip() for idx in top_indices]
        top_preds_per_mask.append(', '.join(top_tokens))

    # Return predictions from the first mask token
    return top_preds_per_mask[0] if top_preds_per_mask else ''

# # Apply the function to get predictions
# df['top_pred_5'] = df.apply(lambda row: get_predictions_with_mask(row['sentence'], row['target']), axis=1)

def check_target_in_predictions(predictions, target):
    # Split predictions into a list and check if target is in the list
    return 1 if target in predictions.split(', ') else 0

def check_target_is_top_prediction(predictions, target):
    # Split predictions into a list and check if target is in the list
    return 1 if target in predictions.split(', ')[0] else 0

# # Check if the target word is in the predictions
# df['in_top_pred_5'] = df.apply(lambda row: check_target_in_predictions(row['top_pred_5'], row['target']), axis=1)

# # Check if the target word is top 1 prediction
# df['in_top_pred_1'] = df.apply(lambda row: check_target_is_top_prediction(row['top_pred_5'], row['target']), axis=1)

"""# Prepare data"""

df1 = pd.read_csv('/content/drive/MyDrive/LLM_role-reversal/files/WY_clean.csv').reset_index(drop=True)
df2 = pd.read_csv('/content/drive/MyDrive/LLM_role-reversal/files/LE_clean.csv').reset_index(drop=True)
df3 = pd.read_csv('/content/drive/MyDrive/LLM_role-reversal/files/KO_clean.csv').reset_index(drop=True)
df3.loc[(df3.condition == 'canonical')|(df3.condition == 'reversed'), 'type'] = 'reversal'
df3.loc[(df3.condition == 'high-cloze')|(df3.condition == 'low-cloze'), 'type'] = 'control'
df1['exp'] = 'WY'
df2['exp'] = 'LE'
df2['item'] = df2['item'] + 120
df2.loc[df2['type'] == 'alternative', 'item' ] = df2['item'] + 60
df3['exp'] = 'KO'
df3['item'] = df3['item'] + 240
df3.loc[df3['type'] == 'control', 'item' ] = df3['item'] + 96
df = pd.concat([df1,df2,df3],ignore_index = True)

df = df[['exp','item','sentence','context_had','type','condition','patient','agent','target']]
df.loc[(df['condition'] == 'canonical')|(df['condition'] == 'high-cloze')|(df['condition'] == 'alt_canonical'), 'plausibility'] = 'plausible'
df.loc[(df['condition'] == 'reversed')|(df['condition'] == 'low-cloze')|(df['condition'] == 'alt_reversed'), 'plausibility'] = 'implausible'

df.to_csv(f'/content/drive/MyDrive/LLM_role-reversal/files/df_comb.csv')

df

"""# Apply functions and get results

"""

model_name = 'gpt2'
model = AutoModelForCausalLM.from_pretrained(f'{model_name}', return_dict=True)
tokenizer = AutoTokenizer.from_pretrained(f'{model_name}', use_fast=True)

df['top_pred_5'] = df['context_had'].apply(get_top_predictions)
df['in_top_pred_5'] = df.apply(lambda row: check_target_in_predictions(row['top_pred_5'], row['target']), axis=1)
df['in_top_pred_1'] = df.apply(lambda row: check_target_is_top_prediction(row['top_pred_5'], row['target']), axis=1)
df['model_name'] = model_name

df.to_csv(f'/content/drive/MyDrive/LLM_role-reversal/files/toppred_{model_name}.csv', index=False)

model_name = 'gpt2-medium'
model = AutoModelForCausalLM.from_pretrained(f'{model_name}', return_dict=True)
tokenizer = AutoTokenizer.from_pretrained(f'{model_name}', use_fast=True)

df['top_pred_5'] = df['context_had'].apply(get_top_predictions)
df['in_top_pred_5'] = df.apply(lambda row: check_target_in_predictions(row['top_pred_5'], row['target']), axis=1)
df['in_top_pred_1'] = df.apply(lambda row: check_target_is_top_prediction(row['top_pred_5'], row['target']), axis=1)
df['model_name'] = model_name

df.to_csv(f'/content/drive/MyDrive/LLM_role-reversal/files/toppred_{model_name}.csv', index=False)

model_name = 'gpt2-large'
model = AutoModelForCausalLM.from_pretrained(f'{model_name}', return_dict=True)
tokenizer = AutoTokenizer.from_pretrained(f'{model_name}', use_fast=True)

df['top_pred_5'] = df['context_had'].apply(get_top_predictions)
df['in_top_pred_5'] = df.apply(lambda row: check_target_in_predictions(row['top_pred_5'], row['target']), axis=1)
df['in_top_pred_1'] = df.apply(lambda row: check_target_is_top_prediction(row['top_pred_5'], row['target']), axis=1)
df['model_name'] = model_name

df.to_csv(f'/content/drive/MyDrive/LLM_role-reversal/files/toppred_{model_name}.csv', index=False)

model_name = 'bert-base-uncased'
bert_model = AutoModelForCausalLM.from_pretrained(f'{model_name}', return_dict=True)
bert_tokenizer = AutoTokenizer.from_pretrained(f'{model_name}', use_fast=True)
model = scorer.IncrementalLMScorer(bert_model, tokenizer=bert_tokenizer, device='cpu')

df['top_pred_5'] = df.apply(lambda row: get_predictions_with_mask(row['sentence'], row['target']), axis=1)
df['in_top_pred_5'] = df.apply(lambda row: check_target_in_predictions(row['top_pred_5'], row['target']), axis=1)
df['in_top_pred_1'] = df.apply(lambda row: check_target_is_top_prediction(row['top_pred_5'], row['target']), axis=1)
df['model_name'] = model_name

df.to_csv(f'/content/drive/MyDrive/LLM_role-reversal/files/toppred_{model_name}.csv', index=False)

model_name = 'bert-large-uncased'
bert_model = AutoModelForCausalLM.from_pretrained(f'{model_name}', return_dict=True)
bert_tokenizer = AutoTokenizer.from_pretrained(f'{model_name}', use_fast=True)
model = scorer.IncrementalLMScorer(bert_model, tokenizer=bert_tokenizer, device='cpu')

df['top_pred_5'] = df.apply(lambda row: get_predictions_with_mask(row['sentence'], row['target']), axis=1)
df['in_top_pred_5'] = df.apply(lambda row: check_target_in_predictions(row['top_pred_5'], row['target']), axis=1)
df['in_top_pred_1'] = df.apply(lambda row: check_target_is_top_prediction(row['top_pred_5'], row['target']), axis=1)
df['model_name'] = model_name

df.to_csv(f'/content/drive/MyDrive/LLM_role-reversal/files/toppred_{model_name}.csv', index=False)

model_name = 'roberta-base'
bert_model = AutoModelForCausalLM.from_pretrained(f'{model_name}', return_dict=True)
bert_tokenizer = AutoTokenizer.from_pretrained(f'{model_name}', use_fast=True)
model = scorer.IncrementalLMScorer(bert_model, tokenizer=bert_tokenizer, device='cpu')

df['top_pred_5'] = df.apply(lambda row: get_predictions_with_mask(row['sentence'], row['target']), axis=1)
df['in_top_pred_5'] = df.apply(lambda row: check_target_in_predictions(row['top_pred_5'], row['target']), axis=1)
df['in_top_pred_1'] = df.apply(lambda row: check_target_is_top_prediction(row['top_pred_5'], row['target']), axis=1)
df['model_name'] = model_name

df.to_csv(f'/content/drive/MyDrive/LLM_role-reversal/files/toppred_{model_name}.csv', index=False)

model_name = 'roberta-large'
bert_model = AutoModelForCausalLM.from_pretrained(f'{model_name}', return_dict=True)
bert_tokenizer = AutoTokenizer.from_pretrained(f'{model_name}', use_fast=True)
model = scorer.IncrementalLMScorer(bert_model, tokenizer=bert_tokenizer, device='cpu')

df['top_pred_5'] = df.apply(lambda row: get_predictions_with_mask(row['sentence'], row['target']), axis=1)
df['in_top_pred_5'] = df.apply(lambda row: check_target_in_predictions(row['top_pred_5'], row['target']), axis=1)
df['in_top_pred_1'] = df.apply(lambda row: check_target_is_top_prediction(row['top_pred_5'], row['target']), axis=1)
df['model_name'] = model_name

df.to_csv(f'/content/drive/MyDrive/LLM_role-reversal/files/toppred_{model_name}.csv', index=False)

model_name = 'EleutherAI/gpt-j-6B'
model = AutoModelForCausalLM.from_pretrained(f'{model_name}', return_dict=True)
tokenizer = AutoTokenizer.from_pretrained(f'{model_name}', use_fast=True)

df['top_pred_5'] = df['context_had'].apply(get_top_predictions)
df['in_top_pred_5'] = df.apply(lambda row: check_target_in_predictions(row['top_pred_5'], row['target']), axis=1)
df['in_top_pred_1'] = df.apply(lambda row: check_target_is_top_prediction(row['top_pred_5'], row['target']), axis=1)
df['model_name'] = model_name

df.to_csv(f'/content/drive/MyDrive/LLM_role-reversal/files/toppred_{model_name}.csv', index=False)

from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained("EleutherAI/gpt-j-6B")
model = AutoModelForCausalLM.from_pretrained("EleutherAI/gpt-j-6B")

"""## Load top predictions results"""

model_name = 'gpt2'
df_toppred1 = pd.read_csv(f'/content/drive/MyDrive/LLM_role-reversal/files/toppred_{model_name}.csv')
model_name = 'gpt2-medium'
df_toppred2 = pd.read_csv(f'/content/drive/MyDrive/LLM_role-reversal/files/toppred_{model_name}.csv')
model_name = 'gpt2-large'
df_toppred3 = pd.read_csv(f'/content/drive/MyDrive/LLM_role-reversal/files/toppred_{model_name}.csv')
model_name = 'bert-base-uncased'
df_toppred4 = pd.read_csv(f'/content/drive/MyDrive/LLM_role-reversal/files/toppred_{model_name}.csv')
model_name = 'bert-large-uncased'
df_toppred5 = pd.read_csv(f'/content/drive/MyDrive/LLM_role-reversal/files/toppred_{model_name}.csv')
model_name = 'roberta-base'
df_toppred6 = pd.read_csv(f'/content/drive/MyDrive/LLM_role-reversal/files/toppred_{model_name}.csv')
model_name = 'roberta-large'
df_toppred7 = pd.read_csv(f'/content/drive/MyDrive/LLM_role-reversal/files/toppred_{model_name}.csv')
# model_name = 'EleutherAI/gpt-j-6B'
# df_toppred8 = pd.read_csv(f'/content/drive/MyDrive/LLM_role-reversal/files/toppred_{model_name}.csv')

df_toppred_comb = pd.concat([df_toppred1,df_toppred2,df_toppred3,df_toppred4,df_toppred5,df_toppred6,df_toppred7], ignore_index=True).drop_duplicates()
df_toppred_comb.loc[(df_toppred_comb['condition'] == 'canonical')|(df_toppred_comb['condition'] == 'alt_canonical')|(df_toppred_comb['condition'] == 'high-cloze'), 'plausibility'] = 'plausible'
df_toppred_comb.loc[(df_toppred_comb['condition'] == 'reversed')|(df_toppred_comb['condition'] == 'alt_reversed')|(df_toppred_comb['condition'] == 'low-cloze'), 'plausibility'] = 'implausible'

df_toppred_comb.loc[(df_toppred_comb['exp'] == 'WY')&(df_toppred_comb['type'] == 'reversal'),'experiment'] = 'CSLP_role'
df_toppred_comb.loc[(df_toppred_comb['exp'] == 'WY')&(df_toppred_comb['type'] == 'substitution'),'experiment'] = 'CSLP_sub'

df_toppred_comb.loc[(df_toppred_comb['exp'] == 'LE')&(df_toppred_comb['type'] == 'reversal'),'experiment'] = 'ELP_role1'
df_toppred_comb.loc[(df_toppred_comb['exp'] == 'LE')&(df_toppred_comb['type'] == 'alternative'),'experiment'] = 'ELP_role2'

df_toppred_comb.loc[(df_toppred_comb['exp'] == 'KO')&(df_toppred_comb['type'] == 'reversal'),'experiment'] = 'KO_role'
df_toppred_comb.loc[(df_toppred_comb['exp'] == 'KO')&(df_toppred_comb['type'] == 'control'),'experiment'] = 'KO_control'

df_toppred_comb

g = sns.catplot(
    df_toppred_comb, kind="bar",
    x="model_name", y="in_top_pred_1", col = 'experiment', hue = 'plausibility', errorbar = None
)#.set(ylim=(0, .3))
g.set(xticklabels=["gpt2-s", "gpt2-m", "gpt2-l", "bert-b", "bert-l", "roberta-b", "roberta-l"])
plt.show()

g = sns.catplot(
    df_toppred_comb, kind="bar",
    x="model_name", y="in_top_pred_5", col = 'experiment', hue = 'plausibility', errorbar = None
)#.set(ylim=(0, .6))
g.set(xticklabels=["gpt2-s", "gpt2-m", "gpt2-l", "bert-b", "bert-l", "roberta-b", "roberta-l"])
plt.show()

