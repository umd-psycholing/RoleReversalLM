# -*- coding: utf-8 -*-
"""top_predictions.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zZ9fh78_GBeLB6vbkt4edvGZnORQBBYM
"""

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForMaskedLM

def get_top_predictions(model, tokenizer, context, sentence, target_word=None, model_type='gpt'):
    if 'gpt' in model_type:
        # Encode the input context
        input_ids = tokenizer.encode(context, return_tensors='pt')

        # Get logits from the model
        with torch.no_grad():
            outputs = model(input_ids)
            logits = outputs.logits

        # Extract probabilities for the last token
        last_logits = logits[0, -1, :]
        probs = torch.softmax(last_logits, dim=-1)

        # Get the top 5 predictions
        top_probs, top_indices = torch.topk(probs, 5)
        top_tokens = [tokenizer.decode([idx]).strip() for idx in top_indices]
        return ', '.join(top_tokens)  # Join predictions into a single string

    elif 'bert' in model_type or 'roberta' in model_type:
        if target_word is None:
            raise ValueError("For BERT model, 'target_word' must be provided.")

        # Replace the target word with the mask token
        words = sentence.split()
        masked_text = ' '.join([tokenizer.mask_token if word.lower() == target_word.lower() else word for word in words])

        # Debugging print statements
        # print(f"Original sentence: {sentence}")
        # print(f"Masked sentence: {masked_text}")

        input_ids = tokenizer.encode(masked_text, return_tensors='pt')

        # Get logits from the model
        with torch.no_grad():
            outputs = model(input_ids)
            logits = outputs.logits

        # Extract probabilities for the mask token
        mask_token_index = torch.where(input_ids == tokenizer.mask_token_id)[1]
        # Multiple mask tokens might be present; handle this case
        mask_logits = logits[0, mask_token_index, :]
        top_preds_per_mask = []
        for logits in mask_logits:
            probs = torch.softmax(logits, dim=-1)
            top_probs, top_indices = torch.topk(probs, 5)
            top_tokens = [tokenizer.decode([idx]).strip() for idx in top_indices]
            top_preds_per_mask.append(', '.join(top_tokens))

        # Return predictions from the first mask token
        return top_preds_per_mask[0] if top_preds_per_mask else ''

    else:
        raise ValueError("Invalid model type. Supported types are 'gpt' and 'bert'.")

def check_target_in_predictions(predictions, target):
    # Split predictions into a list and check if target is in the list
    return 1 if target in predictions.split(', ') else 0

def check_target_is_top_prediction(predictions, target):
    # Split predictions into a list and check if target is the top prediction
    return 1 if target == predictions.split(', ')[0] else 0